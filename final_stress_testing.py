import asyncio
import pandas as pd
import numpy as np
import uuid
import sys
import os
import warnings
import logging
import argparse
import json
import time
from datetime import datetime, timezone
from typing import Tuple, List, Dict, Any

# 1. Load Env & Config
from dotenv import load_dotenv
load_dotenv()

from google.cloud import bigquery
from google import genai
from google.genai import types
from google.auth import default as google_auth_default
from google.adk.sessions import InMemorySessionService
from google.adk.agents import InvocationContext, RunConfig
from google.genai.types import Content, Part
from colorama import Fore, Style, init

# Initialize
warnings.filterwarnings('ignore')
init(autoreset=True)

# Logging Setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.FileHandler("stress_test.log")]
)
logger = logging.getLogger(__name__)

# ==========================================
# SETUP & IMPORTS
# ==========================================
project_root = os.path.abspath(os.path.dirname(__file__))
sys.path.insert(0, project_root)

try:
    from config.settings import settings
    from agent import root_agent
except ImportError as e:
    print(f"{Fore.RED}‚ùå Critical Error: Could not load agent/settings. {e}{Style.RESET_ALL}")
    sys.exit(1)

# ==========================================
# COMPONENT 1: THE STRONG GEMINI JUDGE
# ==========================================
class SemanticDataJudge:
    def __init__(self):
        # FIX: Set default region to asia-south1
        location = getattr(settings, 'vertex_ai_location', 'asia-south1')
        api_key = getattr(settings, 'google_api_key', None)
        
        if api_key:
            self.client = genai.Client(api_key=api_key)
        else:
            self.client = genai.Client(vertexai=True, project=settings.gcp_project_id, location=location)

        self.model_name = getattr(settings, 'gemini_pro_model', 'gemini-1.5-pro')
        
    def evaluate(self, question: str, df_ref: pd.DataFrame, df_bot: pd.DataFrame) -> Tuple[bool, str]:
        # TIER 1: STRICT MATCH (Fastest)
        if df_ref.empty and df_bot.empty: return True, "Exact Match (Both Empty)"
        if df_ref.empty: return False, "Fail: Reference is Empty but Bot returned data"
        if df_bot.empty: return False, "Fail: Bot returned No Data"

        try:
            # Check shape first
            if df_ref.shape == df_bot.shape:
                # Try strict numeric comparison
                ref_nums = df_ref.select_dtypes(include='number').values
                bot_nums = df_bot.select_dtypes(include='number').values
                if ref_nums.shape == bot_nums.shape:
                    # strict check
                    if np.allclose(ref_nums, bot_nums, equal_nan=True):
                        return True, "Numeric Exact Match"
        except:
            pass

        # TIER 2: STRONG LLM JUDGE
        # Convert to Markdown for LLM reading
        # We take head(20) to give the LLM enough context but save tokens
        ref_str = df_ref.head(20).to_markdown(index=False)
        bot_str = df_bot.head(20).to_markdown(index=False)

        prompt = f"""
        Act as a **Senior Data Quality Engineer**. Compare the "Ground Truth Data" vs "Candidate Data" generated by an AI SQL Agent.
        
        User Question: "{question}"

        ### Ground Truth Data (Correct):
        {ref_str}

        ### Candidate Data (To Evaluate):
        {bot_str}

        ### Evaluation Criteria (Strict):
        1. **Semantic Equivalence:** Do the values convey the exact same meaning? (e.g., "50%" == "0.5", "USD 100" == "100")
        2. **Column Mapping:** Ignore column names (e.g., 'total_sales' vs 'sum_sales' is OK). Look at the *values* in the columns.
        3. **Row Composition:** Does the candidate have the same number of rows? (Ignore sort order unless specific ranking was asked).
        4. **Precision:** Allow for small floating point differences (e.g. 100.01 vs 100.02 is PASS).
        5. **Extra Data:** If Candidate has *extra* helpful columns but includes the core correct data, it is a PASS.
        
        ### Output Format:
        Return ONLY valid JSON:
        {{
            "match": boolean, 
            "reason": "concise explanation of failure or success",
            "failure_type": "None" | "Data_Mismatch" | "Empty_Result" | "Hallucination"
        }}
        """

        try:
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                    temperature=0.0 
                )
            )
            
            # Robust JSON extraction
            txt = response.text if hasattr(response, 'text') else str(response)
            clean_json = txt.replace('```json', '').replace('```', '').strip()
            result = json.loads(clean_json)
            
            return result.get("match", False), f"LLM: {result.get('reason', 'N/A')}"
            
        except Exception as e:
            return False, f"Judge Error: {str(e)}"

# ==========================================
# COMPONENT 2: BIGQUERY EXECUTOR (Fixed)
# ==========================================
class BigQueryExecutor:
    def __init__(self, project_id: str):
        self.client = bigquery.Client(project=project_id)
        self.project_id = project_id
        self.dataset_id = settings.logging_dataset
        self.table_id = "evaluation_history_v4"

    def run(self, sql: str) -> Tuple[pd.DataFrame, str, float]:
        if not sql or not isinstance(sql, str):
            return pd.DataFrame(), "No SQL Generated", 0.0

        start_ts = time.time()
        try:
            clean_sql = sql.replace('```sql', '').replace('```', '').strip()
            df = self.client.query(clean_sql).to_dataframe()
            # Normalize headers for comparison
            df.columns = [str(c).lower().strip() for c in df.columns] 
            return df, "", (time.time() - start_ts)
        except Exception as e:
            return pd.DataFrame(), str(e), (time.time() - start_ts)

    def log_batch(self, results: List[Dict]):
        """
        FIXED: Uses load_table_from_dataframe and ensures directory exists.
        """
        if not results: return

        table_ref = f"{self.project_id}.{self.dataset_id}.{self.table_id}"
        logger.info(f"üì§ Uploading {len(results)} rows to {table_ref}...")

        try:
            df = pd.DataFrame(results)

            # --- PRE-UPLOAD CLEANING ---
            # 1. Force timestamps to datetime
            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            # 2. Force strings to prevent object errors
            str_cols = ['question', 'expected_domain', 'actual_domain', 'status', 'reason', 'generated_sql', 'error_message']
            for c in str_cols:
                if c in df.columns:
                    df[c] = df[c].astype(str)

            # 3. Handle Boolean (BigQuery prefers nullable bools differently)
            # We let pandas handle it, but ensure no objects
            
            # --- UPLOAD ---
            job_config = bigquery.LoadJobConfig(
                write_disposition="WRITE_APPEND",
                schema_update_options=[bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION],
                time_partitioning=bigquery.TimePartitioning(type_=bigquery.TimePartitioningType.DAY, field="timestamp")
            )

            job = self.client.load_table_from_dataframe(df, table_ref, job_config=job_config)
            job.result()
            
            logger.info(f"‚úÖ BQ Success: {len(results)} rows.")
            print(f"{Fore.GREEN}‚úÖ Logged to BigQuery ({table_ref}){Style.RESET_ALL}")

        except Exception as e:
            logger.error(f"‚ùå BQ Load Failed: {e}")
            print(f"{Fore.RED}‚ùå BQ Upload Failed: {e}{Style.RESET_ALL}")
            # Fallback CSV dump
            try:
                fail_file = f"reports/bq_failed_upload_{int(time.time())}.csv"
                os.makedirs(os.path.dirname(fail_file), exist_ok=True)
                df.to_csv(fail_file, index=False)
                print(f"   Saved failed rows to {fail_file}")
            except:
                pass

# ==========================================
# COMPONENT 3: ROBUST AGENT RUNNER (Retries + Tokens)
# ==========================================
from google.api_core import exceptions as google_exceptions

async def get_agent_response(user_question: str) -> Dict[str, Any]:
    """
    Robust Agent Runner:
    1. Retries on 429 (Quota) and 503 (Service Unavailable) errors.
    2. Captures Input/Output Token usage for cost tracking.
    3. Returns a standardized Dictionary.
    """
    max_retries = 3
    base_delay = 2  # seconds

    for attempt in range(max_retries):
        try:
            session_service = InMemorySessionService()
            session = await session_service.create_session(
                session_id=str(uuid.uuid4()), app_name="stress-test", user_id="admin"
            )

            ctx = InvocationContext(
                session_service=session_service,
                invocation_id=str(uuid.uuid4()),
                agent=root_agent,
                session=session,
                user_content=Content(parts=[Part(text=user_question)], role="user"),
                run_config=RunConfig()
            )

            # Capture the last response to get token metadata
            last_response = None
            async for response in root_agent.run_async(ctx):
                last_response = response

            # 1. Extract Metrics (Token Usage)
            # Handle cases where usage_metadata might be None
            usage = getattr(last_response, 'usage_metadata', None)
            input_tokens = getattr(usage, 'prompt_token_count', 0) if usage else 0
            output_tokens = getattr(usage, 'candidates_token_count', 0) if usage else 0

            # 2. Extract SQL & Domain
            sql = ""
            domain = "Unknown"
            
            # Safe State Extraction
            if 'sql_generation_response' in ctx.session.state:
                data = ctx.session.state['sql_generation_response']
                if isinstance(data, dict):
                    sql = data.get('sql_query') or data.get('generated_sql') or ""
                else:
                    sql = getattr(data, 'sql_query', "") or getattr(data, 'generated_sql', "")

            if 'routing_response' in ctx.session.state:
                data = ctx.session.state['routing_response']
                if isinstance(data, dict):
                    domain = data.get('selected_domain', 'Unknown')
                else:
                    domain = getattr(data, 'selected_domain', 'Unknown')

            return {
                "sql": str(sql).strip(),
                "domain": str(domain),
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "error": None
            }

        except (google_exceptions.ResourceExhausted, google_exceptions.ServiceUnavailable) as e:
            # Exponential Backoff for Rate Limiting
            if attempt < max_retries - 1:
                sleep_time = base_delay * (2 ** attempt)
                logger.warning(f"‚ö†Ô∏è API Busy (Attempt {attempt+1}). Retrying in {sleep_time}s...")
                await asyncio.sleep(sleep_time)
            else:
                return {"sql": "", "domain": "Unknown", "input_tokens": 0, "output_tokens": 0, "error": f"Max Retries Exceeded: {e}"}
        except Exception as e:
            # Fatal crash, do not retry
            return {"sql": "", "domain": "Unknown", "input_tokens": 0, "output_tokens": 0, "error": f"Crash: {e}"}

# ==========================================
# MAIN LOOP (Optimized + Rate Limiting)
# ==========================================
async def run_stress_test(runs: int = 3):
    if not os.path.exists('golden_question_bank.csv'):
        print(f"{Fore.RED}‚ùå Missing golden_question_bank.csv{Style.RESET_ALL}")
        return

    # Ensure reports directory
    os.makedirs('reports', exist_ok=True)

    executor = BigQueryExecutor(settings.gcp_project_id)
    judge = SemanticDataJudge()
    df_gold = pd.read_csv('golden_question_bank.csv')
    
    batch_id = uuid.uuid4().hex[:6]
    all_results = []

    print(f"{Fore.CYAN}üöÄ Starting CAIDO Reliability Test (Batch: {batch_id}){Style.RESET_ALL}")
    print(f"Testing {len(df_gold)} Questions x {runs} Iterations | Region: asia-south1\n")

    for idx, row in df_gold.iterrows():
        qid = idx + 1
        question = row.get('Questions')
        ref_sql = row.get('reference_sql')
        exp_domain = row.get('Domain', 'Unknown')
        
        if pd.isna(question): continue

        print(f"{Style.BRIGHT}üîπ Q{qid}: {question[:50]}...{Style.RESET_ALL}")

        # 1. Get Ground Truth
        ref_df, ref_err, _ = executor.run(ref_sql)
        if ref_err:
            print(f"   {Fore.RED}‚ö†Ô∏è Bad Reference SQL: {ref_err[:30]}{Style.RESET_ALL}")

        # 2. Iteration Loop
        q_sqls = [] 
        sys.stdout.write("   Runs: ")
        
        for i in range(1, runs + 1):
            # A. Agent (with Retries & Token Counting)
            agent_res = await get_agent_response(question)
            
            bot_sql = agent_res['sql']
            bot_domain = agent_res['domain']
            
            # B. Execute (Only if agent succeeded)
            if agent_res['error']:
                bot_df = pd.DataFrame()
                bot_err = agent_res['error']
                duration = 0.0
            else:
                bot_df, bot_err, duration = executor.run(bot_sql)
            
            # C. Evaluate
            status = "FAIL"
            reason = ""
            
            if ref_err: status, reason = "SKIP", "Ref Broken"
            elif bot_err: status, reason = "ERROR", f"Error: {bot_err[:30]}"
            else:
                match, r = judge.evaluate(question, ref_df, bot_df)
                status = "PASS" if match else "FAIL"
                reason = r

            # Console visual
            if status == "PASS": sys.stdout.write(f"{Fore.GREEN}‚Ä¢{Style.RESET_ALL}")
            elif status == "ERROR": sys.stdout.write(f"{Fore.RED}E{Style.RESET_ALL}")
            else: sys.stdout.write(f"{Fore.YELLOW}x{Style.RESET_ALL}")
            sys.stdout.flush()

            q_sqls.append(bot_sql)
            
            # D. Log Result (Now includes Tokens)
            all_results.append({
                "run_id": batch_id,
                "timestamp": datetime.now(timezone.utc),
                "question_id": int(qid),
                "run_number": int(i),
                "question": question,
                "expected_domain": str(exp_domain),
                "actual_domain": str(bot_domain),
                "status": status,
                "reason": reason,
                "latency_seconds": float(round(duration, 2)),
                "input_tokens": int(agent_res['input_tokens']),
                "output_tokens": int(agent_res['output_tokens']),
                "generated_sql": bot_sql,
                "error_message": bot_err
            })

            # E. Rate Limiting (Pacing)
            # Sleep 1 second to prevent hitting "Requests Per Minute" quota
            await asyncio.sleep(1.0)

        # Calculate Consistency
        unique_count = len(set(q_sqls))
        consistency_pct = 100 if unique_count == 1 else (100 - ((unique_count-1)/runs * 100))
        print(f" | Consist: {int(consistency_pct)}%")

    # ==========================================
    # REPORTING & SAVING
    # ==========================================
    if all_results:
        df_res = pd.DataFrame(all_results)
        
        # 1. Save Raw Data
        csv_path = f"reports/stress_raw_{batch_id}.csv"
        df_res.to_csv(csv_path, index=False)
        
        # 2. Generate Summary Matrix (Added Token Costs)
        df_grouped = df_res.groupby('question_id').agg({
            'status': lambda x: (x == 'PASS').mean() * 100,
            'latency_seconds': 'mean',
            'input_tokens': 'mean',  # Track avg cost
            'output_tokens': 'mean', # Track avg cost
            'generated_sql': lambda x: x.nunique(),
            'question': 'first',
            'reason': lambda x: x.mode()[0] if not x.mode().empty else 'N/A'
        }).reset_index()
        
        df_grouped['consistency_score'] = 1 - ((df_grouped['generated_sql'] - 1) / runs)
        df_grouped['consistency_score'] = df_grouped['consistency_score'].clip(lower=0) * 100

        # Print Executive Summary
        print(f"\n{Fore.WHITE}{'='*100}")
        print(f"üìä CAIDO EXECUTIVE SUMMARY (Batch {batch_id})")
        print(f"{'='*100}{Style.RESET_ALL}")
        
        print(f"{'ID':<3} | {'Pass %':<7} | {'Lat(s)':<6} | {'Tokens(I/O)':<12} | {'Consist%':<8} | {'Reason'}")
        print("-" * 100)
        
        for _, row in df_grouped.iterrows():
            p = row['status']
            c = row['consistency_score']
            tok = f"{int(row['input_tokens'])}/{int(row['output_tokens'])}"
            color = Fore.GREEN if p > 80 and c > 80 else (Fore.RED if p < 50 else Fore.YELLOW)
            print(f"{row['question_id']:<3} | {color}{p:>5.1f}%{Style.RESET_ALL} | {row['latency_seconds']:>6.2f} | {tok:<12} | {c:>7.0f}%  | {str(row['reason'])[:30]}")

        # 3. Save Markdown Report
        md_path = f"reports/stress_summary_{batch_id}.md"
        with open(md_path, "w") as f:
            f.write(f"# Stress Test Report - Batch {batch_id}\n")
            f.write(f"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n")
            f.write(df_grouped[['question_id', 'question', 'status', 'latency_seconds', 'input_tokens', 'consistency_score', 'reason']].to_markdown(index=False))

        print(f"\n{Fore.CYAN}üìÅ Files Saved:{Style.RESET_ALL}")
        print(f"   1. Raw Data: {csv_path}")
        print(f"   2. Summary:  {md_path}")
        
        # 4. Upload to BigQuery
        executor.log_batch(all_results)

# ==========================================
# EXECUTION ENTRY POINT (PASTE AT VERY BOTTOM)
# ==========================================
if __name__ == "__main__":
    # 1. Argument Parsing
    parser = argparse.ArgumentParser(description="Stress Test the AI Agent")
    parser.add_argument("--runs", type=int, default=3, help="Number of times to ask each question")
    args = parser.parse_args()

    # 2. Debug Print (To confirm it's starting)
    print(f"{Fore.CYAN}üîå Orchestrator loaded. Initializing Stress Test with {args.runs} runs...{Style.RESET_ALL}")

    # 3. Run Async Loop
    try:
        asyncio.run(run_stress_test(args.runs))
    except KeyboardInterrupt:
        print(f"\n{Fore.YELLOW}üõë Test interrupted by user.{Style.RESET_ALL}")
    except Exception as e:
        print(f"\n{Fore.RED}‚ùå Fatal Script Error: {e}{Style.RESET_ALL}")